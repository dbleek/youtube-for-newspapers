{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0dca1e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sparknlp in /home/ob2205/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: pymongo in /home/ob2205/.local/lib/python3.10/site-packages (4.6.3)\n",
      "Requirement already satisfied: spark-nlp in /home/ob2205/.local/lib/python3.10/site-packages (from sparknlp) (5.3.3)\n",
      "Requirement already satisfied: numpy in /ext3/pyspark/lib/python3.10/site-packages (from sparknlp) (1.23.4)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/ob2205/.local/lib/python3.10/site-packages (from pymongo) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sparknlp pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4d3eb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient    \n",
    "import pyspark\n",
    "from pyspark.ml import PipelineModel\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import time\n",
    "import re\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873d634",
   "metadata": {},
   "source": [
    "# Writing to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b8df6809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /state/partition1/job-46084207/ipykernel_1641172/2555205902.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [361], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.mongodb.spark:mongo-spark-connector_2.12:3.0.1,com.databricks:spark-xml_2.12:0.18.0,com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8g\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSQLContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m      7\u001b[0m spark\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/pyspark/context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    427\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    435\u001b[0m             currentAppName,\n\u001b[1;32m    436\u001b[0m             currentMaster,\n\u001b[1;32m    437\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    438\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    439\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    440\u001b[0m         )\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /state/partition1/job-46084207/ipykernel_1641172/2555205902.py:5 "
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.jars.packages', \n",
    "         \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,com.databricks:spark-xml_2.12:0.18.0,com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\")\n",
    "conf.set('spark.driver.memory','8g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uri = f\"mongodb+srv://dmb443:{os.environ['MONGODB_PASS']}@bigdatafinalproject.sl03s.mongodb.net/?retryWrites=true&w=majority&appName=BigDataFinalProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e285c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=client.bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62014c-dfa6-499c-9756-81accc1a4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4c548090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract articles from zip\n",
    "\n",
    "# source_path = \"/scratch/work/public/proquest/proquest_hnp/BostonGlobe/BG_20151210212722_00001.zip\"\n",
    "# with ZipFile(source_path, \"r\") as zip:\n",
    "#     zip.extractall('zip_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a275855",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.load.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:404)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:136)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat com.databricks.spark.xml.XmlRelation.<init>(XmlRelation.scala:39)\n\tat com.databricks.spark.xml.XmlRelation$.apply(XmlRelation.scala:29)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:80)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:52)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrootTag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrowTag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecursiveFileLookup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip_tmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.load.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:404)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:136)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat com.databricks.spark.xml.XmlRelation.<init>(XmlRelation.scala:39)\n\tat com.databricks.spark.xml.XmlRelation$.apply(XmlRelation.scala:29)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:80)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:52)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .option('rootTag', 'Record')\\\n",
    "    .option('rowTag', 'Record')\\\n",
    "    .option('recursiveFileLookup', 'true')\\\n",
    "    .format(\"xml\").load(\"zip_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb24e7-bdac-4508-ad24-ae674942a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SparkNLP\n",
    "spark = sparknlp.start() # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3449541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 5.3.3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SQLContext' object has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark NLP version\u001b[39m\u001b[38;5;124m\"\u001b[39m, sparknlp\u001b[38;5;241m.\u001b[39mversion())\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApache Spark version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SQLContext' object has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaff13-6727-47a1-816b-56b361a0a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YakePipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    A pipeline for extracting keywords using YAKE.\n",
    "\n",
    "    Example:\n",
    "    pipeline = YakePipeline()\n",
    "    processed_df = pipeline.fit(df).transfrom(df)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(YakePipeline, self).__init__()\n",
    "        self.stopwords = StopWordsCleaner().getStopWords()\n",
    "        self.document = DocumentAssembler() \\\n",
    "                .setInputCol(\"FullText\") \\\n",
    "                .setOutputCol(\"document\")\n",
    "        self.sentenceDetector = SentenceDetector() \\\n",
    "                .setInputCols(\"document\") \\\n",
    "                .setOutputCol(\"sentence\")\n",
    "        self.token = Tokenizer() \\\n",
    "                .setInputCols(\"sentence\") \\\n",
    "                .setOutputCol(\"token\") \\\n",
    "                .setContextChars([\"(\", \")\", \"?\", \"!\", \".\", \",\"])\n",
    "        self.keywords = YakeKeywordExtraction() \\\n",
    "                .setInputCols(\"token\") \\\n",
    "                .setOutputCol(\"keywords\") \\\n",
    "                .setMinNGrams(1) \\\n",
    "                .setMaxNGrams(3)\\\n",
    "                .setNKeywords(20)\\\n",
    "                .setStopWords(self.stopwords)\n",
    "        self.setStages([self.document, self.sentenceDetector, self.token, self.keywords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcbac2-7be8-4111-becc-73d53e4b5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords and append to df\n",
    "yake_pipeline = YakePipeline()\n",
    "\n",
    "result = yake_pipeline.fit(df).transform(df)\\\n",
    "    .drop(\"document\")\\\n",
    "    .drop(\"token\")\\\n",
    "    .drop(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b2f22-1c8c-4a26-938d-7003a3ac0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df now includes keywords, along with other outputs from pipeline\n",
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422c146-9f8e-4271-92d2-4c23c09e2acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write to mongo\n",
    "start = time.time()\n",
    "result.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"database\", \"bigdata\")\\\n",
    "    .option(\"collection\", \"newspapers\")\\\n",
    "    .option(\"uri\", uri)\\\n",
    "    .save()\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bad5a",
   "metadata": {},
   "source": [
    "# Querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "bed55dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query, stopwords):\n",
    "    \"\"\"Takes an user query and stopwords array and returns a keywords array\"\"\"\n",
    "    query = re.sub(\"[^a-z]\", \" \", query.strip().lower())\n",
    "    query = re.sub(\"  +\", \" \", query)\n",
    "    keywords = query.split()\n",
    "    keywords = [word for word in keywords if word not in stopwords]\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def keyword_search(keywords, db, k):\n",
    "    \"\"\"Takes keywords array and returns top-k articles with those keywords.\"\"\"\n",
    "    # aggregate pipeline to find top-k articles based on input keywords\n",
    "    data = db.aggregate([\n",
    "        {\"$match\": {\"ObjectType\": \"Article\"}}, # Filter only articles\n",
    "        {\"$project\":{\"keywords.result\": 1, \"keywords.metadata.score\": 1}},\n",
    "        {\"$unwind\": {\"path\": \"$keywords\", \"preserveNullAndEmptyArrays\": False}},\n",
    "        {\"$match\": {\"keywords.result\":  {\"$in\": keywords}}},\n",
    "        {\"$group\": { \"_id\": \"$_id\", \"keywords\": {\"$addToSet\": \"$keywords\"}}},\n",
    "        {\"$unwind\": {\"path\": \"$keywords\", \"preserveNullAndEmptyArrays\": False}},\n",
    "        # TODO: Zero to large number\n",
    "        {\"$addFields\": {\"keywords.reciprocal\": {\"$divide\": [1, {\"$convert\": {\"input\":\"$keywords.metadata.score\", \"to\": \"double\", \"onError\": 10**7, \"onNull\": 10**7}}]}}},\n",
    "        {\"$group\": { \"_id\": \"$_id\", \"score\": {\"$sum\": \"$keywords.reciprocal\"},\"keywords\": {\"$addToSet\": \"$keywords.result\"}}},\n",
    "        {\"$sort\": {\"score\": -1}},\n",
    "        {\"$limit\": k}\n",
    "    ])\n",
    "    \n",
    "    # TO DO\n",
    "    # Optimize pipeline to remove redundant groupBys and/or unwinds\n",
    "    # Remove duplicates\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcd3cc",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3e3c8b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".box_style{\n",
       "    width:100%;\n",
       "    border : None;\n",
       "    height: auto;\n",
       "    background-color:#EEE;\n",
       "    color=white;\n",
       "}\n",
       ".side_bar{\n",
       "    width:100%;\n",
       "    border: None;\n",
       "    height: auto;\n",
       "    background-color:#66b2b2;\n",
       "    color=white;\n",
       "}\n",
       "\n",
       ".widget-label {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       ".widget_text {\n",
       "    border-radius: 8px;\n",
       "}\n",
       ".button_style {\n",
       "    margin-top: 15px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".box_style{\n",
    "    width:100%;\n",
    "    border : None;\n",
    "    height: auto;\n",
    "    background-color:#EEE;\n",
    "    color=white;\n",
    "}\n",
    ".side_bar{\n",
    "    width:100%;\n",
    "    border: None;\n",
    "    height: auto;\n",
    "    background-color:#66b2b2;\n",
    "    color=white;\n",
    "}\n",
    "\n",
    ".widget-label {\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    ".widget_text {\n",
    "    border-radius: 8px;\n",
    "}\n",
    ".button_style {\n",
    "    margin-top: 15px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4815b738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebd38fc0e184682a2a1a5f9c9c87ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import HTML, display, clear_output, Javascript\n",
    "from bson.objectid import ObjectId\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def show_document(header, text):\n",
    "    \n",
    "    display(Javascript(\"\"\"\n",
    "        require(\n",
    "            [\"base/js/dialog\"], \n",
    "            function(dialog) {\n",
    "                console.log('d', dialog.modal);\n",
    "                dialog.modal({\n",
    "                    title: '%s',\n",
    "                    body: 'replace_me',\n",
    "                    buttons: {\n",
    "                        'Done': {}\n",
    "                    }\n",
    "                });\n",
    "                // Using setTimeout to wait for modal to render first\n",
    "                setTimeout(function(){\n",
    "                const found_modals = document.getElementsByClassName(\"modal-body\");\n",
    "                for( const fm of found_modals){\n",
    "                    if(fm.innerText === 'replace_me'){\n",
    "                        fm.innerHTML = \"<div>%s</div>\";\n",
    "                    }\n",
    "                }\n",
    "                }, 300)\n",
    "            }\n",
    "        );\n",
    "        \"\"\" % (header, text)))\n",
    "\n",
    "def view_doc_clicked(title, rendered_text, _b):\n",
    "    show_document(title, rendered_text)\n",
    "\n",
    "\n",
    "notify_output = ipw.Output()\n",
    "display(notify_output)\n",
    "\n",
    "@notify_output.capture()\n",
    "def popup(text):\n",
    "    clear_output()\n",
    "    display(HTML(\"<script>alert('{}');</script>\".format(text)))\n",
    "\n",
    "def create_result_option(record_id, record_title, ctx_text, publisher, pub_date, object_id, rendered_text):\n",
    "    items_layout = ipw.Layout(width='90%')\n",
    "    \n",
    "    children = []\n",
    "    row_layout = ipw.Layout(display='flex', flex_flow='row', align_items='stretch', border_bottom='solid 2px lightgrey', padding='5px')\n",
    "    sn_box = ipw.HBox([ipw.HTML(f\"{record_id}.\")], layout=ipw.Layout(width='3%', margin='5px 5px 5px 5px'))\n",
    "    rn_box = ipw.HBox(children=[], layout=ipw.Layout(width='10%', margin='5px 5px 5px 5px'))\n",
    "    \n",
    "    if len(record_title):\n",
    "        btnView = ipw.Button(description=\"View\", tooltip=str(object_id))\n",
    "        btnView.on_click(partial(view_doc_clicked, record_title, rendered_text))\n",
    "        rn_box.children = [btnView]\n",
    "        \n",
    "        \n",
    "        children.append(ipw.HTML(f\"<b><font color='#1b75d0'; size=3px>{record_title}\", layout=items_layout))\n",
    "    if len(ctx_text):\n",
    "        children.append(ipw.HTML(ctx_text, layout=items_layout))\n",
    "    if len(publisher):\n",
    "        children.append(ipw.HTML(f\"<font color='grey'><b>Publisher:</b> {publisher}\", layout=items_layout))\n",
    "    if len(pub_date):\n",
    "        children.append(ipw.HTML(f\"<font color='grey'><b>Date:</b> {pub_date}\", layout=items_layout))\n",
    "    \n",
    "    row_content = ipw.VBox(children = children, layout=ipw.Layout(width='90%'))\n",
    "    row = ipw.HBox(children=[sn_box, row_content, rn_box], layout=row_layout)\n",
    " \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2df89790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        text = re.sub(\"[^0-9a-zA-Z]\", \" \", text)\n",
    "        text = re.sub(\" +\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "def article_keyword_highlighter(fulltext, keyword, chunk_start=None, chunk_end=None):\n",
    "    output = []\n",
    "    chunk_end = chunk_end if chunk_end else len(fulltext)\n",
    "    chunk_start = chunk_start if chunk_start else 0\n",
    "    block = fulltext[chunk_start:chunk_end]\n",
    "    m = re.finditer(keyword, block, re.IGNORECASE)\n",
    "    ind = 0\n",
    "    for f in m:\n",
    "        sp = f.span()\n",
    "        start = sp[0]\n",
    "        end = sp[1]\n",
    "        output.append(block[ind:start] + \\\n",
    "            f\"<span style='background-color: yellow;'>{block[start:end]}</span>\")\n",
    "        ind = end\n",
    "    output.append(block[ind:])\n",
    "    return \"\".join(output)\n",
    "\n",
    "def word_highlighter(fulltext, keyword, ctx_length=100):\n",
    "    tot_length = len(fulltext)\n",
    "    f = re.search(keyword, fulltext, re.IGNORECASE)\n",
    "    if not f:\n",
    "        return fulltext\n",
    "    sp = f.span()\n",
    "    start = sp[0]\n",
    "    end = sp[1]\n",
    "    chunk_start = 0 if start-ctx_length < 0 else start-ctx_length\n",
    "    chunk_end = tot_length if end+ctx_length > tot_length else end+ctx_length\n",
    "    return article_keyword_highlighter(fulltext, keyword, chunk_start, chunk_end)\n",
    "\n",
    "def search(query, num_articles=5):\n",
    "    \n",
    "    stopwords = StopWordsCleaner().getStopWords()\n",
    "    keywords = clean_query(query, stopwords)\n",
    "    keywords_regex = \"|\".join(keywords)\n",
    "    \n",
    "    documents_cursor = keyword_search(keywords, db.newspapers, num_articles)\n",
    "    \n",
    "    document_ids = list(doc[\"_id\"] for doc in documents_cursor)\n",
    "    \n",
    "    data = db.newspapers.find({\"_id\": {\"$in\": document_ids}})\n",
    "    \n",
    "    data = list(data)\n",
    "    i = 1\n",
    "    children = []\n",
    "    if len(data) < 1:\n",
    "        children.append(create_result_option(\"#\", '', \"No articles found with this query\", \"\", '', ''))\n",
    "        return children\n",
    "\n",
    "    for d in data:\n",
    "        text = clean_text(d[\"FullText\"])\n",
    "        ctx_text= word_highlighter(text, keywords_regex)\n",
    "\n",
    "        rendered_text = article_keyword_highlighter(text, keywords_regex)\n",
    "        children.append(create_result_option(i,\n",
    "                                             d[\"RecordTitle\"],\n",
    "                                             ctx_text,\n",
    "                                             d[\"Publisher\"],\n",
    "                                             d[\"AlphaPubDate\"],\n",
    "                                             d['_id'],\n",
    "                                            rendered_text)\n",
    "                       )\n",
    "        i += 1\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e747845a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c321787fa9d14b12a6cd79e65f3f1d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(HBox(children=(HTML(value='<h1> YouTube for Newspapers </h1>', layout=Layout(align_content…"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyError",
     "evalue": "'Abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [357], line 20\u001b[0m, in \u001b[0;36mon_button_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     18\u001b[0m     popup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter search Key.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     main_panel\u001b[38;5;241m.\u001b[39mchildren \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_keyword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [356], line 64\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query, num_articles)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m children\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 64\u001b[0m     abstract \u001b[38;5;241m=\u001b[39m clean_text(\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAbstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     65\u001b[0m     text \u001b[38;5;241m=\u001b[39m clean_text(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFullText\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     66\u001b[0m     ctx_text\u001b[38;5;241m=\u001b[39m word_highlighter(abstract, keywords_regex)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Abstract'"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [357], line 20\u001b[0m, in \u001b[0;36mon_button_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     18\u001b[0m     popup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter search Key.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     main_panel\u001b[38;5;241m.\u001b[39mchildren \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_keyword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [356], line 64\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query, num_articles)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m children\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 64\u001b[0m     abstract \u001b[38;5;241m=\u001b[39m clean_text(\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAbstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     65\u001b[0m     text \u001b[38;5;241m=\u001b[39m clean_text(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFullText\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     66\u001b[0m     ctx_text\u001b[38;5;241m=\u001b[39m word_highlighter(abstract, keywords_regex)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Abstract'"
     ]
    }
   ],
   "source": [
    "hor_layout = ipw.Layout(align_content='stretch', margin='0.1% 1% 0.1% 2% ', width='100%')\n",
    "\n",
    "app_title = ipw.HTML('<h1> YouTube for Newspapers </h1>', layout=hor_layout)\n",
    "app_footer = ipw.HTML('<p> Apache Spark, HDFS, MongoDB, NLP</p>', layout=hor_layout)\n",
    "\n",
    "headerBox = ipw.HBox([app_title]).add_class('box_style')\n",
    "footerBox = ipw.HBox([app_footer]).add_class('box_style')\n",
    "\n",
    "txt_keyword = ipw.Text(placeholder='Enter a Keyword', description='Keyword:', disabled=False, layout=ipw.Layout(width='auto', \n",
    "    margin='15px 10px 5px 2px')).add_class('widget_text')\n",
    "btnSearch = ipw.Button(description=\"Search!\", icon='search').add_class('button_style')\n",
    "box_layout = ipw.Layout(display='flex', flex_flow='column', align_items='center', width='100%')\n",
    "btnContainer = ipw.HBox(children=[btnSearch], layout=box_layout)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    if len(txt_keyword.value.strip()) < 1:\n",
    "        popup(\"Enter search Key.\")\n",
    "    else:\n",
    "        main_panel.children = search(txt_keyword.value.lower())\n",
    "\n",
    "btnSearch.on_click(on_button_clicked)\n",
    "\n",
    "side_bar = ipw.VBox([txt_keyword, btnContainer]).add_class('side_bar')\n",
    "main_panel = ipw.VBox([])\n",
    "\n",
    "ipw.AppLayout(header=headerBox, left_sidebar=side_bar, center=main_panel, right_sidebar=None, footer=footerBox,\n",
    "    pane_widths=[2, 7, 0],\n",
    "    pane_heights=[1, 9, '40px'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047a929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da2d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
