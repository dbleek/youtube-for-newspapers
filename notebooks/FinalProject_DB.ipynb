{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dca1e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sparknlp in /home/ob2205/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: pymongo in /home/ob2205/.local/lib/python3.10/site-packages (4.6.3)\n",
      "Requirement already satisfied: numpy in /ext3/pyspark/lib/python3.10/site-packages (from sparknlp) (1.23.4)\n",
      "Requirement already satisfied: spark-nlp in /home/ob2205/.local/lib/python3.10/site-packages (from sparknlp) (5.3.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/ob2205/.local/lib/python3.10/site-packages (from pymongo) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sparknlp pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3eb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient    \n",
    "import pyspark\n",
    "from pyspark.ml import PipelineModel\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import time\n",
    "import re\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873d634",
   "metadata": {},
   "source": [
    "# Writing to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8df6809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/ext3/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ob2205/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ob2205/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ab33db0-d877-4c88-a9dd-29eb1791849b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound com.databricks#spark-xml_2.12;0.18.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.17.0 in central\n",
      ":: resolution report :: resolve 3634ms :: artifacts dl 78ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.databricks#spark-xml_2.12;0.18.0 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.17.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   92  |   0   |   0   |   5   ||   87  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ab33db0-d877-4c88-a9dd-29eb1791849b\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0 artifacts copied, 87 already retrieved (0kB/33ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/05 09:36:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/ext3/pyspark/lib/python3.10/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x14b03e0c9ed0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.jars.packages', \n",
    "         \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,com.databricks:spark-xml_2.12:0.18.0,com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\")\n",
    "conf.set('spark.driver.memory','8g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "122a86cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MONGODB_PASS=\"wXwfzBEQTangXV44\"\n"
     ]
    }
   ],
   "source": [
    "%env MONGODB_PASS=\"wXwfzBEQTangXV44\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a0acb9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wXwfzBEQTangXV44\r\n"
     ]
    }
   ],
   "source": [
    "! echo $MONGODB_PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8dfe89f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'SHELL': '/bin/bash', 'SLURM_JOB_USER': 'ob2205', 'SLURM_TASKS_PER_NODE': '1(x4)', 'SLURM_JOB_UID': '4089557', 'HISTCONTROL': 'ignoredups', 'SLURM_EXPORT_ENV': 'NONE', 'SLURM_TASK_PID': '1639801', 'CONDA_EXE': '/ext3/miniconda3/bin/conda', '_CE_M': '', 'SPARK_URL': 'spark://cm011:29239', 'SLURM_LOCALID': '0', 'SLURM_SUBMIT_DIR': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad', 'HISTSIZE': '1000', 'HOSTNAME': 'cm011', 'SPARK_LOG_DIR': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad/logs', 'SPARK_SSH_OPTS': '-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR -p 59207', 'SINGULARITY_NAME': 'ubuntu-20.04.3.sif', 'SLURMD_NODENAME': 'cm011', 'FPATH': '/usr/share/zsh/5.5.1/functions:', 'SLURM_JOB_START_TIME': '1714915684', 'HYDRA_LAUNCHER_EXTRA_ARGS': '--external-launcher', 'JAVA_HOME': '/usr/lib/jvm/java-11-openjdk-amd64', 'SPARK_MASTER_HOST': 'cm011', '__LMOD_REF_COUNT_MODULEPATH': '/share/apps/modulefiles:1', 'SPARK_WORKER_PORT': '29239', 'CUSTOM_PYSPARK_OVERLAY': '', 'SLURM_CLUSTER_NAME': '(null)', 'SLURM_JOB_END_TIME': '1714944484', 'SLURM_CPUS_ON_NODE': '16', 'CONFIG_FILE': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad/config.py', 'SINGULARITY_ENVIRONMENT': '/.singularity.d/env/91-environment.sh', 'SLURM_JOB_CPUS_PER_NODE': '16(x4)', 'VAST': '/vast/ob2205', 'LMOD_DIR': '/share/apps/lmod/8.4.9/lmod/lmod/libexec', 'PRTE_MCA_plm_slurm_args': '--external-launcher', 'PWD': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad', 'SLURM_GTIDS': '0', 'GSETTINGS_SCHEMA_DIR': '/ext3/pyspark/share/glib-2.0/schemas', 'LOGNAME': 'ob2205', 'XDG_SESSION_TYPE': 'unspecified', 'CONDA_PREFIX': '/ext3/pyspark', 'SLURM_JOB_PARTITION': 'cm', 'MODULESHOME': '/share/apps/lmod/8.4.9/lmod/lmod', 'SLURM_TRES_PER_TASK': 'cpu:16', 'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '', 'SLURM_JOB_NUM_NODES': '4', 'TZ': 'America/New_York', 'SBATCH_IGNORE_PBS': '1', 'SLURM_JOBID': '46084207', 'BASE': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad', 'I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS': '--external-launcher', 'SLURM_JOB_QOS': 'cpu48', 'USER_PATH': '/home/ob2205/.local/bin:/home/ob2205/bin:/share/apps/singularity/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/share/apps/local/bin:/usr/lpp/mmfs/bin:/opt/slurm/bin:/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin', 'HOME': '/home/ob2205', 'SPARK_WORKER_CORES': '16', '_ModuleTable_Sz_': '1', 'LANG': 'C.UTF-8', 'port': '44217', 'SLURM_PROCID': '0', 'SCRATCH': '/scratch/ob2205', 'SINGULARITY_CONTAINER': '/scratch/work/public/singularity/ubuntu-20.04.3.sif', 'LMOD_SETTARG_FULL_SUPPORT': 'no', 'CONDA_PROMPT_MODIFIER': '(/ext3/pyspark) ', 'TMPDIR': '/state/partition1/job-46084207', 'PROMPT_COMMAND': 'PS1=\"Singularity> \"; unset PROMPT_COMMAND', 'SLURM_CPUS_PER_TASK': '16', 'CPL_ZIP_ENCODING': 'UTF-8', 'LMOD_DISABLE_SAME_NAME_AUTOSWAP': 'yes', 'SLURM_TOPOLOGY_ADDR': 'cm011', 'SPARK_MASTER_PORT': '29239', 'LMOD_VERSION': '8.4.9', 'SPARK_DAEMON_MEMORY': '1G', 'SPARK_PID_DIR': '/state/partition1/job-46084207', 'HYDRA_BOOTSTRAP': 'slurm', 'MLM_LICENSE_FILE': '27000@lm2.its.nyu.edu,27000@lm3.its.nyu.edu,27000@lm4.its.nyu.edu', 'SLURM_TOPOLOGY_ADDR_PATTERN': 'node', 'PYSPARK_PYTHON': '/ext3/pyspark/bin/python3', 'XDG_SESSION_CLASS': 'background', 'LMOD_PKG': '/share/apps/lmod/8.4.9/lmod/lmod', 'SLURM_SCRIPT_CONTEXT': 'prolog_task', 'SLURM_MEM_PER_NODE': '65536', 'PYTHONPATH': '', '_CE_CONDA': '', 'NODELIST': 'cm011\\ncm016\\ncm021\\ncm025', 'LESSOPEN': '||/usr/bin/lesspipe.sh %s', 'USER': 'ob2205', 'SLURM_NODELIST': 'cm[011,016,021,025]', 'ENVIRONMENT': 'BATCH', 'CONDA_SHLVL': '1', 'SLURM_JOB_ACCOUNT': 'users', 'SLURM_PRIO_PROCESS': '0', 'SLURM_TMPDIR': '/state/partition1/job-46084207', 'LMOD_ROOT': '/share/apps/lmod/8.4.9/lmod', 'SHLVL': '5', 'SLURM_NNODES': '4', 'GDAL_DRIVER_PATH': '/ext3/pyspark/lib/gdalplugins', 'SPARK_HOME': '/ext3/spark-3.3.0-bin-hadoop3', 'SPARK_LOCAL_IP': 'cm011', 'PROJ_LIB': '/ext3/pyspark/share/proj', 'SINGULARITY_BIND': '/sys/fs/cgroup,/state/partition1/ob2205/spark-tmp:/tmp,/mnt,/share/apps,/vast', 'XDG_SESSION_ID': 'c2090', 'SPARK_EXECUTOR_MEMORY': '1G', 'SLURM_SUBMIT_HOST': 'ood-4.hpc.nyu.edu', '_ModuleTable001_': 'X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXt9LG1wYXRoQT17Ii9zaGFyZS9hcHBzL21vZHVsZWZpbGVzIix9LH0=', 'CONDA_PYTHON_EXE': '/ext3/miniconda3/bin/python', 'LD_LIBRARY_PATH': '/.singularity.d/libs', 'SPARK_CONF_DIR': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad/conf', 'XDG_RUNTIME_DIR': '/state/partition1/job-46084207', 'SPARK_WORKER_LOG_DIR': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad', 'SLURM_JOB_ID': '46084207', 'SLURM_NODEID': '0', 'S_COLORS': 'auto', 'CONDA_DEFAULT_ENV': '/ext3/pyspark', 'LMOD_OPTIONS': '--quiet', 'which_declare': '', 'LC_ALL': 'C', 'host': 'cm011.hpc.nyu.edu', 'SPARK_WORKER_DIR': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad', 'SSH_PORT': '59207', 'GDAL_DATA': '/ext3/pyspark/share/gdal', 'SLURM_CONF': '/opt/slurm/etc/slurm.conf', 'PATH': '/ext3/spark-3.3.0-bin-hadoop3/bin:/ext3/spark-3.3.0-bin-hadoop3/sbin:/ext3/pyspark/bin:/ext3/miniconda3/bin:.:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SLURM_JOB_NAME': 'ood-spark-cluster', 'LMOD_EXACT_MATCH': 'yes', 'MODULEPATH': '/share/apps/modulefiles', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/4089557/bus', 'SLURM_JOBTMP': '/state/partition1/job-46084207', 'LMOD_CMD': '/share/apps/lmod/8.4.9/lmod/lmod/libexec/lmod', 'MAIL': '/var/spool/mail/ob2205', 'INTEL_LICENSE_FILE': '/share/apps/intel/licenses', 'MEMORY': '1G', 'OMPI_MCA_plm_slurm_args': '--external-launcher', 'SINGULARITY_COMMAND': 'exec', 'SLURM_GET_USER_ENV': '1', 'SLURM_JOB_GID': '4089557', 'PROJ_NETWORK': 'ON', 'OLDPWD': '/home/ob2205/ondemand/data/sys/dashboard/batch_connect/sys/spark-standalone/output/65938138-300b-4ffb-a95a-9d8ba5db0aad', 'CLUSTER': 'GREENE', 'SLURM_JOB_NODELIST': 'cm[011,016,021,025]', 'SPARK_WORKER_MEMORY': '1G', 'I_MPI_HYDRA_BOOTSTRAP': 'slurm', 'BASH_FUNC_ml%%': '() {  eval $($LMOD_DIR/ml_cmd \"$@\")\\n}', 'BASH_FUNC_find_port%%': '() {  local host=\"${1:-localhost}\";\\n local min_port=${2:-2000};\\n local max_port=${3:-65535};\\n local port_range=($(shuf -i ${min_port}-${max_port}));\\n local retries=1;\\n for ((attempt=0; attempt<=$retries; attempt++))\\n do\\n for port in \"${port_range[@]}\";\\n do\\n if port_used \"${host}:${port}\"; then\\n continue;\\n fi;\\n echo \"${port}\";\\n return 0;\\n done;\\n done;\\n echo \"error: failed to find available port in range ${min_port}..${max_port}\" 1>&2;\\n return 1\\n}', 'BASH_FUNC_create_passwd%%': \"() {  tr -cd 'a-zA-Z0-9' < /dev/urandom 2> /dev/null | head -c${1:-8}\\n}\", 'BASH_FUNC_random_number%%': '() {  shuf -i ${1}-${2} -n 1\\n}', 'BASH_FUNC_module%%': '() {  eval $($LMOD_CMD bash \"$@\") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)\\n}', 'BASH_FUNC_wait_until_port_used%%': '() {  local port=\"${1}\";\\n local time=\"${2:-30}\";\\n for ((i=1; i<=time*2; i++))\\n do\\n port_used \"${port}\";\\n port_status=$?;\\n if [ \"$port_status\" == \"0\" ]; then\\n return 0;\\n else\\n if [ \"$port_status\" == \"127\" ]; then\\n echo \"commands to find port were either not found or inaccessible.\";\\n echo \"command options are lsof, nc, bash\\'s /dev/tcp, or python (or python3) with socket lib.\";\\n return 127;\\n fi;\\n fi;\\n sleep 0.5;\\n done;\\n return 1\\n}', 'BASH_FUNC_port_used%%': '() {  local port=\"${1#*:}\";\\n local host=$((expr \"${1}\" : \\'\\\\(.*\\\\):\\' || echo \"localhost\") | awk \\'END{print $NF}\\');\\n local port_strategies=(port_used_nc port_used_lsof port_used_bash port_used_python port_used_python3);\\n for strategy in ${port_strategies[@]};\\n do\\n $strategy $host $port;\\n status=$?;\\n if [[ \"$status\" == \"0\" ]] || [[ \"$status\" == \"1\" ]]; then\\n return $status;\\n fi;\\n done;\\n return 127\\n}', 'BASH_FUNC_source_helpers%%': '() {  function random_number () \\n { \\n shuf -i ${1}-${2} -n 1\\n };\\n export -f random_number;\\n function port_used_python () \\n { \\n python -c \"import socket; socket.socket().connect((\\'$1\\',$2))\" > /dev/null 2>&1\\n };\\n function port_used_python3 () \\n { \\n python3 -c \"import socket; socket.socket().connect((\\'$1\\',$2))\" > /dev/null 2>&1\\n };\\n function port_used_nc () \\n { \\n nc -w 2 \"$1\" \"$2\" < /dev/null > /dev/null 2>&1\\n };\\n function port_used_lsof () \\n { \\n lsof -i :\"$2\" > /dev/null 2>&1\\n };\\n function port_used_bash () \\n { \\n local bash_supported=$(strings /bin/bash 2>/dev/null | grep tcp);\\n if [ \"$bash_supported\" == \"/dev/tcp/*/*\" ]; then\\n ( : < /dev/tcp/$1/$2 ) > /dev/null 2>&1;\\n else\\n return 127;\\n fi\\n };\\n function port_used () \\n { \\n local port=\"${1#*:}\";\\n local host=$((expr \"${1}\" : \\'\\\\(.*\\\\):\\' || echo \"localhost\") | awk \\'END{print $NF}\\');\\n local port_strategies=(port_used_nc port_used_lsof port_used_bash port_used_python port_used_python3);\\n for strategy in ${port_strategies[@]};\\n do\\n $strategy $host $port;\\n status=$?;\\n if [[ \"$status\" == \"0\" ]] || [[ \"$status\" == \"1\" ]]; then\\n return $status;\\n fi;\\n done;\\n return 127\\n };\\n export -f port_used;\\n function find_port () \\n { \\n local host=\"${1:-localhost}\";\\n local min_port=${2:-2000};\\n local max_port=${3:-65535};\\n local port_range=($(shuf -i ${min_port}-${max_port}));\\n local retries=1;\\n for ((attempt=0; attempt<=$retries; attempt++))\\n do\\n for port in \"${port_range[@]}\";\\n do\\n if port_used \"${host}:${port}\"; then\\n continue;\\n fi;\\n echo \"${port}\";\\n return 0;\\n done;\\n done;\\n echo \"error: failed to find available port in range ${min_port}..${max_port}\" 1>&2;\\n return 1\\n };\\n export -f find_port;\\n function wait_until_port_used () \\n { \\n local port=\"${1}\";\\n local time=\"${2:-30}\";\\n for ((i=1; i<=time*2; i++))\\n do\\n port_used \"${port}\";\\n port_status=$?;\\n if [ \"$port_status\" == \"0\" ]; then\\n return 0;\\n else\\n if [ \"$port_status\" == \"127\" ]; then\\n echo \"commands to find port were either not found or inaccessible.\";\\n echo \"command options are lsof, nc, bash\\'s /dev/tcp, or python (or python3) with socket lib.\";\\n return 127;\\n fi;\\n fi;\\n sleep 0.5;\\n done;\\n return 1\\n };\\n export -f wait_until_port_used;\\n function create_passwd () \\n { \\n tr -cd \\'a-zA-Z0-9\\' < /dev/urandom 2> /dev/null | head -c${1:-8}\\n };\\n export -f create_passwd\\n}', '_': '/ext3/pyspark/bin/jupyter', 'JPY_PARENT_PID': '1640379', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'SPARK_AUTH_SOCKET_TIMEOUT': '15', 'SPARK_BUFFER_SIZE': '65536', 'MONGODB_PASS': '\"wXwfzBEQTangXV44\"'})\n"
     ]
    }
   ],
   "source": [
    "print(os.environ)\n",
    "uri = f\"mongodb+srv://dmb443:{os.environ['MONGODB_PASS']}@bigdatafinalproject.sl03s.mongodb.net/?retryWrites=true&w=majority&appName=BigDataFinalProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9eaad924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mongodb+srv://dmb443:wXwfzBEQTangXV44@bigdatafinalproject.sl03s.mongodb.net/?retryWrites=true&w=majority&appName=BigDataFinalProject'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1efd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c33a08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7e285c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '7.0.8',\n",
       " 'gitVersion': 'c5d33e55ba38d98e2f48765ec4e55338d67a4a64',\n",
       " 'modules': ['enterprise'],\n",
       " 'allocator': 'tcmalloc',\n",
       " 'javascriptEngine': 'mozjs',\n",
       " 'sysInfo': 'deprecated',\n",
       " 'versionArray': [7, 0, 8, 0],\n",
       " 'openssl': {'running': 'OpenSSL 1.0.2k-fips  26 Jan 2017',\n",
       "  'compiled': 'OpenSSL 1.0.2k-fips  26 Jan 2017'},\n",
       " 'buildEnvironment': {'distmod': 'amazon2',\n",
       "  'distarch': 'aarch64',\n",
       "  'cc': '/opt/mongodbtoolchain/v4/bin/gcc: gcc (GCC) 11.3.0',\n",
       "  'ccflags': '-Werror -include mongo/platform/basic.h -ffp-contract=off -fasynchronous-unwind-tables -g2 -Wall -Wsign-compare -Wno-unknown-pragmas -Winvalid-pch -gdwarf-5 -fno-omit-frame-pointer -fno-strict-aliasing -O2 -march=armv8.2-a -mtune=generic -Wno-unused-local-typedefs -Wno-unused-function -Wno-deprecated-declarations -Wno-unused-const-variable -Wno-unused-but-set-variable -Wno-missing-braces -fstack-protector-strong -gdwarf64 -Wa,--nocompress-debug-sections -Wimplicit-fallthrough=5',\n",
       "  'cxx': '/opt/mongodbtoolchain/v4/bin/g++: g++ (GCC) 11.3.0',\n",
       "  'cxxflags': '-Woverloaded-virtual -Wpessimizing-move -Wno-maybe-uninitialized -fsized-deallocation -Wno-deprecated -std=c++20',\n",
       "  'linkflags': '-Wl,--fatal-warnings -B/opt/mongodbtoolchain/v4/bin -gdwarf-5 -pthread -Wl,-z,now -fuse-ld=lld -fstack-protector-strong -gdwarf64 -Wl,--build-id -Wl,--hash-style=gnu -Wl,-z,noexecstack -Wl,--warn-execstack -Wl,-z,relro -Wl,--compress-debug-sections=none -Wl,-z,origin -Wl,--enable-new-dtags',\n",
       "  'target_arch': 'aarch64',\n",
       "  'target_os': 'linux',\n",
       "  'cppdefines': 'SAFEINT_USE_INTRINSICS 0 PCRE2_STATIC NDEBUG _XOPEN_SOURCE 700 _GNU_SOURCE _FORTIFY_SOURCE 2 ABSL_FORCE_ALIGNED_ACCESS BOOST_ENABLE_ASSERT_DEBUG_HANDLER BOOST_FILESYSTEM_NO_CXX20_ATOMIC_REF BOOST_LOG_NO_SHORTHAND_NAMES BOOST_LOG_USE_NATIVE_SYSLOG BOOST_LOG_WITHOUT_THREAD_ATTR BOOST_MATH_NO_LONG_DOUBLE_MATH_FUNCTIONS BOOST_SYSTEM_NO_DEPRECATED BOOST_THREAD_USES_DATETIME BOOST_THREAD_VERSION 5'},\n",
       " 'bits': 64,\n",
       " 'debug': False,\n",
       " 'maxBsonObjectSize': 16777216,\n",
       " 'storageEngines': ['devnull', 'inMemory', 'queryable_wt', 'wiredTiger'],\n",
       " 'ok': 1.0,\n",
       " '$clusterTime': {'clusterTime': Timestamp(1714917848, 7),\n",
       "  'signature': {'hash': b'\\x13\\x195\\xb7f\\r\\xbd\\xb9\\x87\\ro*V\\xc6\\x83X\\x19=\"\\xec',\n",
       "   'keyId': 7363034165505163270}},\n",
       " 'operationTime': Timestamp(1714917848, 7)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c11f6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=client.bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce62014c-dfa6-499c-9756-81accc1a4b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['bigdatafinalproject-shard-00-01.sl03s.mongodb.net:27017', 'bigdatafinalproject-shard-00-02.sl03s.mongodb.net:27017', 'bigdatafinalproject-shard-00-00.sl03s.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='BigDataFinalProject', authsource='admin', replicaset='atlas-qlbwa4-shard-0', tls=True), 'bigdata')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c548090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract articles from zip\n",
    "\n",
    "# source_path = \"/scratch/work/public/proquest/proquest_hnp/BostonGlobe/BG_20151210212722_00001.zip\"\n",
    "# with ZipFile(source_path, \"r\") as zip:\n",
    "#     zip.extractall('zip_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a275855",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.load.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:404)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:136)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat com.databricks.spark.xml.XmlRelation.<init>(XmlRelation.scala:39)\n\tat com.databricks.spark.xml.XmlRelation$.apply(XmlRelation.scala:29)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:80)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:52)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrootTag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrowTag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecursiveFileLookup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip_tmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/ext3/pyspark/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o42.load.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:404)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:136)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat com.databricks.spark.xml.XmlRelation.<init>(XmlRelation.scala:39)\n\tat com.databricks.spark.xml.XmlRelation$.apply(XmlRelation.scala:29)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:80)\n\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:52)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/scratch/ob2205/big_data/youtube-for-newspapers/notebooks/zip_tmp\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .option('rootTag', 'Record')\\\n",
    "    .option('rowTag', 'Record')\\\n",
    "    .option('recursiveFileLookup', 'true')\\\n",
    "    .format(\"xml\").load(\"zip_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb24e7-bdac-4508-ad24-ae674942a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SparkNLP\n",
    "spark = sparknlp.start() # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3449541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 5.3.3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SQLContext' object has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark NLP version\u001b[39m\u001b[38;5;124m\"\u001b[39m, sparknlp\u001b[38;5;241m.\u001b[39mversion())\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApache Spark version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SQLContext' object has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaff13-6727-47a1-816b-56b361a0a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YakePipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    A pipeline for extracting keywords using YAKE.\n",
    "\n",
    "    Example:\n",
    "    pipeline = YakePipeline()\n",
    "    processed_df = pipeline.fit(df).transfrom(df)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(YakePipeline, self).__init__()\n",
    "        self.stopwords = StopWordsCleaner().getStopWords()\n",
    "        self.document = DocumentAssembler() \\\n",
    "                .setInputCol(\"FullText\") \\\n",
    "                .setOutputCol(\"document\")\n",
    "        self.sentenceDetector = SentenceDetector() \\\n",
    "                .setInputCols(\"document\") \\\n",
    "                .setOutputCol(\"sentence\")\n",
    "        self.token = Tokenizer() \\\n",
    "                .setInputCols(\"sentence\") \\\n",
    "                .setOutputCol(\"token\") \\\n",
    "                .setContextChars([\"(\", \")\", \"?\", \"!\", \".\", \",\"])\n",
    "        self.keywords = YakeKeywordExtraction() \\\n",
    "                .setInputCols(\"token\") \\\n",
    "                .setOutputCol(\"keywords\") \\\n",
    "                .setMinNGrams(1) \\\n",
    "                .setMaxNGrams(3)\\\n",
    "                .setNKeywords(20)\\\n",
    "                .setStopWords(self.stopwords)\n",
    "        self.setStages([self.document, self.sentenceDetector, self.token, self.keywords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bcbac2-7be8-4111-becc-73d53e4b5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords and append to df\n",
    "yake_pipeline = YakePipeline()\n",
    "\n",
    "result = yake_pipeline.fit(df).transform(df)\\\n",
    "    .drop(\"document\")\\\n",
    "    .drop(\"token\")\\\n",
    "    .drop(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b2f22-1c8c-4a26-938d-7003a3ac0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df now includes keywords, along with other outputs from pipeline\n",
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422c146-9f8e-4271-92d2-4c23c09e2acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write to mongo\n",
    "start = time.time()\n",
    "result.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"database\", \"bigdata\")\\\n",
    "    .option(\"collection\", \"newspapers\")\\\n",
    "    .option(\"uri\", uri)\\\n",
    "    .save()\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bad5a",
   "metadata": {},
   "source": [
    "# Querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bed55dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query, stopwords):\n",
    "    \"\"\"Takes an user query and stopwords array and returns a keywords array\"\"\"\n",
    "    query = re.sub(\"[^a-z]\", \" \", query.strip().lower())\n",
    "    query = re.sub(\"  +\", \" \", query)\n",
    "    keywords = query.split()\n",
    "    keywords = [word for word in keywords if word not in stopwords]\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def keyword_search(keywords, db, k):\n",
    "    \"\"\"Takes keywords array and returns top-k articles with those keywords.\"\"\"\n",
    "    # aggregate pipeline to find top-k articles based on input keywords\n",
    "    data = db.aggregate([\n",
    "        {\"$project\":{\"keywords.result\": 1, \"keywords.metadata.score\": 1}},\n",
    "        {\"$unwind\": {\"path\": \"$keywords\", \"preserveNullAndEmptyArrays\": False}},\n",
    "        {\"$match\": {\"keywords.result\":  {\"$in\": keywords}}},\n",
    "        {\"$group\": { \"_id\": \"$_id\", \"keywords\": {\"$addToSet\": \"$keywords\"}}},\n",
    "        {\"$unwind\": {\"path\": \"$keywords\", \"preserveNullAndEmptyArrays\": False}},\n",
    "        # TODO: Zero to large number\n",
    "        {\"$addFields\": {\"keywords.reciprocal\": {\"$divide\": [1, {\"$convert\": {\"input\":\"$keywords.metadata.score\", \"to\": \"double\", \"onError\": 10**7, \"onNull\": 10**7}}]}}},\n",
    "        {\"$group\": { \"_id\": \"$_id\", \"score\": {\"$sum\": \"$keywords.reciprocal\"},\"keywords\": {\"$addToSet\": \"$keywords.result\"}}},\n",
    "        {\"$sort\": {\"score\": -1}},\n",
    "        {\"$limit\": k}\n",
    "    ])\n",
    "    \n",
    "    # TO DO\n",
    "    # Optimize pipeline to remove redundant groupBys and/or unwinds\n",
    "    # Remove duplicates\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ec9808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'query', 'historical', 'newspaper', 'search', 'engine']\n"
     ]
    }
   ],
   "source": [
    "stopwords = StopWordsCleaner().getStopWords()\n",
    "print(clean_query(\"This is a test query for our Historical Newspaper Search Engine!\", stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c80d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keyword_search([\"china\", \"green\"], db.newspapers, 5)\n",
    "\n",
    "document_ids = list(doc[\"_id\"] for doc in output)\n",
    "document_ids\n",
    "# for doc in output:\n",
    "#     pprint(doc)\n",
    "\n",
    "articles = db.newspapers.find({\"_id\": {\"$in\": document_ids}})\n",
    "# list(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "289d3b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"welcome to beijing, <span style='background-color: yellow;'>china</span>! <span style='background-color: yellow;'>china</span> is a large country in asia! <span style='background-color: yellow;'>china</span><span style='background-color: yellow;'>china</span>\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# children = []\n",
    "# # print(list(articles))\n",
    "\n",
    "# for d in articles:\n",
    "#     # ctx_txt: take keyword with highest score and contexualize it\n",
    "# #     start = d[\"keywords\"]['begin'] or 0\n",
    "# #     end = d[\"keywords\"]['end'] or 0\n",
    "# #     ctx_text=word_highlighter(d[\"FullText\"], start, end, 90)\n",
    "\n",
    "#     children.append(create_result_option(i, d[\"RecordTitle\"], ctx_text, d[\"Publisher\"], d[\"AlphaPubDate\"], d['_id']))\n",
    "\n",
    "\n",
    "# m = re.finditer(f'china', \"china 123\")\n",
    "# for f in m:\n",
    "#     print(f.span())\n",
    "\n",
    "def word_highlighter_2(fulltext, keyword):\n",
    "    m = re.finditer(f'{keyword}', fulltext)\n",
    "    output = []\n",
    "    tot_length = len(fulltext)\n",
    "    chunk_start = 0\n",
    "    for f in m:\n",
    "        sp = f.span()\n",
    "        start = sp[0]\n",
    "        end = sp[1]\n",
    "        output.append(fulltext[chunk_start:start] + \\\n",
    "            f\"<span style='background-color: yellow;'>{fulltext[start:end]}</span>\")\n",
    "        chunk_start = end\n",
    "    return \"\".join(output)\n",
    "\n",
    "# def word_highlighter(fulltext, keyword, ctx_length=100):\n",
    "#     m = re.finditer(f'{keyword}', fulltext)\n",
    "#     f = next(m)\n",
    "#     sp = f.span()\n",
    "#     start = sp[0]\n",
    "#     end = sp[1]\n",
    "#     tot_length = len(fulltext)\n",
    "#     chunk_start = 0 if start-ctx_length < 0 else start-ctx_length\n",
    "#     chunk_end = tot_length if end+ctx_length > tot_length else end+ctx_length\n",
    "#     return fulltext[chunk_start:start] + \\\n",
    "#         f\"<span style='background-color: yellow;'>{fulltext[start:end + 1]}</span>\" + \\\n",
    "#         fulltext[end + 1:chunk_end]\n",
    "\n",
    "# def article_keyword_highlighter(fulltext, keyword):\n",
    "#     m = re.finditer(f'{keyword}', fulltext)\n",
    "#     output = []\n",
    "#     tot_length = len(fulltext)\n",
    "#     chunk_start = 0\n",
    "#     for f in m:\n",
    "#         sp = f.span()\n",
    "#         start = sp[0]\n",
    "#         end = sp[1]\n",
    "#         output.append(fulltext[chunk_start:] + \\\n",
    "#             f\"<span style='background-color: yellow;'>{fulltext[start:end]}</span>\")\n",
    "#         chunk_start = end\n",
    "#     return \"\".join(output)\n",
    "\n",
    "word_highlighter_2(\"welcome to beijing, china! china is a large country in asia! chinachina test\", \"china\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b7d585d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc(a,b,c):\n",
    "    return a + b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "647d19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "abc2 = partial(abc, 1)\n",
    "\n",
    "print(abc2(4,5))\n",
    "print(abc2(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a1b71e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "76b06343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"modal\">\n",
       "          <div class=\"modal-dialog\">\n",
       "            <div class=\"modal-content\">\n",
       "              <div class=\"modal-header\">\n",
       "                <h4 class=\"modal-title\">This is a modal</h4>\n",
       "              </div>\n",
       "              <div class=\"modal-body\">\n",
       "                This is the content of the modal.\n",
       "              </div>\n",
       "              <div class=\"modal-footer\">\n",
       "                <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n",
       "              </div>\n",
       "            </div>\n",
       "          </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"modal\">\n",
       "          <div class=\"modal-dialog\">\n",
       "            <div class=\"modal-content\">\n",
       "              <div class=\"modal-header\">\n",
       "                <h4 class=\"modal-title\">This is a modal</h4>\n",
       "              </div>\n",
       "              <div class=\"modal-body\">\n",
       "                This is the content of the modal.\n",
       "              </div>\n",
       "              <div class=\"modal-footer\">\n",
       "                <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n",
       "              </div>\n",
       "            </div>\n",
       "          </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "class ModalDialog(object):\n",
    "    def __init__(self, title, content):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "\n",
    "    def show(self):\n",
    "        display(HTML('''\n",
    "        <div class=\"modal\">\n",
    "          <div class=\"modal-dialog\">\n",
    "            <div class=\"modal-content\">\n",
    "              <div class=\"modal-header\">\n",
    "                <h4 class=\"modal-title\">{}</h4>\n",
    "              </div>\n",
    "              <div class=\"modal-body\">\n",
    "                {}\n",
    "              </div>\n",
    "              <div class=\"modal-footer\">\n",
    "                <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n",
    "              </div>\n",
    "            </div>\n",
    "          </div>\n",
    "        </div>\n",
    "        '''.format(self.title, self.content)))\n",
    "\n",
    "# Create a new modal dialog\n",
    "modal_dialog = ModalDialog('This is a modal', 'This is the content of the modal.')\n",
    "modal_dialog.show()\n",
    "# # Show the modal dialog\n",
    "modal_dialog.show()\n",
    "\n",
    "# display(HTML('<b>Hi!</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcd3cc",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3e3c8b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".box_style{\n",
       "    width:100%;\n",
       "    border : None;\n",
       "    height: auto;\n",
       "    background-color:#EEE;\n",
       "    color=white;\n",
       "}\n",
       ".side_bar{\n",
       "    width:100%;\n",
       "    border: None;\n",
       "    height: auto;\n",
       "    background-color:#66b2b2;\n",
       "    color=white;\n",
       "}\n",
       "\n",
       ".widget-label {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       ".widget_text {\n",
       "    border-radius: 8px;\n",
       "}\n",
       ".button_style {\n",
       "    margin-top: 15px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".box_style{\n",
    "    width:100%;\n",
    "    border : None;\n",
    "    height: auto;\n",
    "    background-color:#EEE;\n",
    "    color=white;\n",
    "}\n",
    ".side_bar{\n",
    "    width:100%;\n",
    "    border: None;\n",
    "    height: auto;\n",
    "    background-color:#66b2b2;\n",
    "    color=white;\n",
    "}\n",
    "\n",
    ".widget-label {\n",
    "    color: white !important;\n",
    "}\n",
    "\n",
    ".widget_text {\n",
    "    border-radius: 8px;\n",
    "}\n",
    ".button_style {\n",
    "    margin-top: 15px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4815b738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d9bee6bc744c97bbb43c953e34ef91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import HTML, display, clear_output, Javascript\n",
    "from bson.objectid import ObjectId\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def show_document(header, text):\n",
    "    \n",
    "    display(Javascript(\"\"\"\n",
    "        require(\n",
    "            [\"base/js/dialog\"], \n",
    "            function(dialog) {\n",
    "                console.log('d', dialog.modal);\n",
    "                dialog.modal({\n",
    "                    title: '%s',\n",
    "                    body: 'replace_me',\n",
    "                    buttons: {\n",
    "                        'Done': {}\n",
    "                    }\n",
    "                });\n",
    "                // Using setTimeout to wait for modal to render first\n",
    "                setTimeout(function(){\n",
    "                const found_modals = document.getElementsByClassName(\"modal-body\");\n",
    "                for( const fm of found_modals){\n",
    "                    if(fm.innerText === 'replace_me'){\n",
    "                        fm.innerHTML = \"<div>%s</div>\";\n",
    "                    }\n",
    "                }\n",
    "                }, 300)\n",
    "            }\n",
    "        );\n",
    "        \"\"\" % (header, text)))\n",
    "\n",
    "def view_doc_clicked(title, rendered_text, _b):\n",
    "    show_document(title, rendered_text)\n",
    "\n",
    "\n",
    "notify_output = ipw.Output()\n",
    "display(notify_output)\n",
    "\n",
    "@notify_output.capture()\n",
    "def popup(text):\n",
    "    clear_output()\n",
    "    display(HTML(\"<script>alert('{}');</script>\".format(text)))\n",
    "\n",
    "def create_result_option(record_id, record_title, ctx_text, publisher, pub_date, object_id, rendered_text):\n",
    "    items_layout = ipw.Layout(width='90%')\n",
    "    \n",
    "    children = []\n",
    "    row_layout = ipw.Layout(display='flex', flex_flow='row', align_items='stretch', border_bottom='solid 2px lightgrey', padding='5px')\n",
    "    sn_box = ipw.HBox([ipw.HTML(f\"{record_id}.\")], layout=ipw.Layout(width='3%', margin='5px 5px 5px 5px'))\n",
    "    rn_box = ipw.HBox(children=[], layout=ipw.Layout(width='10%', margin='5px 5px 5px 5px'))\n",
    "    \n",
    "    if len(record_title):\n",
    "        btnView = ipw.Button(description=\"View\", tooltip=str(object_id))\n",
    "        btnView.on_click(partial(view_doc_clicked, record_title, rendered_text))\n",
    "        rn_box.children = [btnView]\n",
    "        \n",
    "        \n",
    "        children.append(ipw.HTML(f\"<b><font color='#1b75d0'; size=3px>{record_title}\", layout=items_layout))\n",
    "    if len(ctx_text):\n",
    "        children.append(ipw.HTML(ctx_text, layout=items_layout))\n",
    "    if len(publisher):\n",
    "        children.append(ipw.HTML(f\"<font color='grey'><b>Publisher:</b> {publisher}\", layout=items_layout))\n",
    "    if len(pub_date):\n",
    "        children.append(ipw.HTML(f\"<font color='grey'><b>Date:</b> {pub_date}\", layout=items_layout))\n",
    "    \n",
    "    row_content = ipw.VBox(children = children, layout=ipw.Layout(width='90%'))\n",
    "    row = ipw.HBox(children=[sn_box, row_content, rn_box], layout=row_layout)\n",
    " \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2df89790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        text = re.sub(\"[^0-9a-zA-Z]\", \" \", text)\n",
    "        text = re.sub(\" +\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "def article_keyword_highlighter(fulltext, keyword):\n",
    "    m = re.finditer(keyword, fulltext, re.IGNORECASE)\n",
    "    output = []\n",
    "    tot_length = len(fulltext)\n",
    "    chunk_start = 0\n",
    "    for f in m:\n",
    "        sp = f.span()\n",
    "        start = sp[0]\n",
    "        end = sp[1]\n",
    "        output.append(fulltext[chunk_start:start] + \\\n",
    "            f\"<span style='background-color: yellow;'>{fulltext[start:end]}</span>\")\n",
    "        chunk_start = end\n",
    "    if not output:\n",
    "        return fulltext\n",
    "    return \"\".join(output)\n",
    "\n",
    "def search(keywords, num_articles=5):\n",
    "    \n",
    "    stopwords = StopWordsCleaner().getStopWords()\n",
    "    \n",
    "    documents_cursor = keyword_search([keyword], db.newspapers, num_articles)\n",
    "    \n",
    "    document_ids = list(doc[\"_id\"] for doc in documents_cursor)\n",
    "    \n",
    "    data = db.newspapers.find({\"_id\": {\"$in\": document_ids}})\n",
    "    \n",
    "    data = list(data)\n",
    "    i = 1\n",
    "    children = []\n",
    "    if len(data) < 1:\n",
    "        children.append(create_result_option(\"#\", '', \"No Newspaper found with this keyword\", \"\", '', ''))\n",
    "        return children\n",
    "\n",
    "    for d in data:\n",
    "        text = clean_text(d[\"FullText\"])\n",
    "        ctx_text= word_highlighter(text, keyword)\n",
    "\n",
    "        rendered_text = article_keyword_highlighter(text, keyword )\n",
    "        children.append(create_result_option(i,\n",
    "                                             d[\"RecordTitle\"],\n",
    "                                             ctx_text,\n",
    "                                             d[\"Publisher\"],\n",
    "                                             d[\"AlphaPubDate\"],\n",
    "                                             d['_id'],\n",
    "                                            rendered_text)\n",
    "                       )\n",
    "        i += 1\n",
    "    return children\n",
    "\n",
    "def word_highlighter(fulltext, keyword, ctx_length=100):\n",
    "    f = re.search(f'{keyword}', fulltext, re.IGNORECASE)\n",
    "    sp = f.span()\n",
    "    start = sp[0]\n",
    "    end = sp[1]\n",
    "    tot_length = len(fulltext)\n",
    "    chunk_start = 0 if start-ctx_length < 0 else start-ctx_length\n",
    "    chunk_end = tot_length if end+ctx_length > tot_length else end+ctx_length\n",
    "    return fulltext[chunk_start:start] + \\\n",
    "        f\"<span style='background-color: yellow;'>{fulltext[start:end + 1]}</span>\" + \\\n",
    "        fulltext[end + 1:chunk_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e747845a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf6408fe42f4b73a8a48f5b36eaf6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(HBox(children=(HTML(value='<h1> YouTube for Newspapers </h1>', layout=Layout(align_content…"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        require(\n",
       "            [\"base/js/dialog\"], \n",
       "            function(dialog) {\n",
       "                console.log('d', dialog.modal);\n",
       "                dialog.modal({\n",
       "                    title: 'Display Ad 12 -- No Title',\n",
       "                    body: 'replace_me',\n",
       "                    buttons: {\n",
       "                        'Done': {}\n",
       "                    }\n",
       "                });\n",
       "                setTimeout(function(){\n",
       "                const found_modals = document.getElementsByClassName(\"modal-body\");\n",
       "                console.log(found_modals);\n",
       "                for( const fm of found_modals){\n",
       "                    console.log('fm', fm)\n",
       "                    if(fm.innerText === 'replace_me'){\n",
       "                        fm.innerHTML = \"<div>MITCHELL WOODBURY COMPANY The House That Is Known by the Customers It Keeps SEMI ANNUAL CLEARANCE SALE BEGINS TODAY Thousands of Dollars Worth of Dinnerware Glass and <span style='background-color: yellow;'>China</span> taken from our regular stock including discontinued patterns and odd lots marked at prices that mean a saving ranging from 25 to 75 from regular prices Hundreds of other equally attractive items not advertised can be found at our store during this sale Service or 12 Person 100 pieces 39 50 Service for 12 Persons 100 pieces 48 00 66 pieces Service for 8 Persons 66 Pieces s20 00 <span style='background-color: yellow;'>China</span> Dinner Service For 8 Persons 66 Pieces S44 25 Ovenglass Baking Set Set consists of fireproof casserole 8 inch size bread baker 10J4 inch size 9 inch pie plate t and six 4 ounce custards Set 9 pieces 2 50 Rich Cut Glass Fruit Bowl Deep brilliant cutting 0 1 finest heavy crystal 8 inch size 2 95 Dinner Sets Special bargains in many of our patterns greatly reduced for this sale All first quality imported semi porcelain Service for 6 persons 35 pieces t 11 25 Service for 6 persons 41 pieces 14 50 Service for 6 persons 38 pieces j 15 75 Service for 8 persons 66 piece 23 85 Service for 8 persons 66 pieces 25 50 Bargains in Decorated Cups and Saucers Plates etc Former Prico Cups and Saucers i 6 50 doz Dessert Plates 4 00 doz Dinner Plates 6 50 dox Meat Platters 1 50 each Fruit Saucers 2 00 doz Oatmeal Saucers 6 00 doz Soup Plates 6 30 doz Now 25 20c each 30c each 75c each 16c each 25c each 20c each 25c WILL BUY OvengUss Pie Plates 8 inch size Japanese Decorated Candlesticks Japanese Brown Teapots individual Light Cut Glass Baskets <span style='background-color: yellow;'>China</span> Tea Cups and Saucers 6 patterns Decorated Glass Vases 7 inch size Japanese Blue and White Teapots 2 cup light Cut Glass Vinegar Cruets <span style='background-color: yellow;'>China</span> Mayonnaise Sets 2 pieces 50c WILL BUY Imported <span style='background-color: yellow;'>China</span> Bon Bon Dishes Ovenglass Bakers lYz qL size Imported <span style='background-color: yellow;'>China</span> Whipped Cream Sets Light Cut Glass Vases 9 inch size Ovenglass Custard Cups set of six Imported Blue and White Teapots 4 cap Imported Bread and Butter Plates set of six 75c WILL BUY Imported <span style='background-color: yellow;'>China</span> Cake Plates 9 inch size Ovenglass Covered Casseroles 1 qt size Imported <span style='background-color: yellow;'>China</span> Sugar and Cream Sets Brown or Green Glazed Teapots 6 cup Ovenglass Bread Bakers lOV j inch size Colored Glass Baskets 6 inch size Imported <span style='background-color: yellow;'>China</span> Marmalade Jars 1 00 WILL BUY Cut Glass Handled Sandwich Plates Imported <span style='background-color: yellow;'>China</span> Bon Bon Imported <span style='background-color: yellow;'>China</span></div>\";\n",
       "                    }\n",
       "                }\n",
       "                }, 300)\n",
       "            }\n",
       "        );\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hor_layout = ipw.Layout(align_content='stretch', margin='0.1% 1% 0.1% 2% ', width='100%')\n",
    "\n",
    "app_title = ipw.HTML('<h1> YouTube for Newspapers </h1>', layout=hor_layout)\n",
    "app_footer = ipw.HTML('<p> Apache Spark, HDFS, MongoDB, NLP</p>', layout=hor_layout)\n",
    "\n",
    "headerBox = ipw.HBox([app_title]).add_class('box_style')\n",
    "footerBox = ipw.HBox([app_footer]).add_class('box_style')\n",
    "\n",
    "txt_keyword = ipw.Text(placeholder='Enter a Keyword', description='Keyword:', disabled=False, layout=ipw.Layout(width='auto', \n",
    "    margin='15px 10px 5px 2px')).add_class('widget_text')\n",
    "btnSearch = ipw.Button(description=\"Search!\", icon='search').add_class('button_style')\n",
    "box_layout = ipw.Layout(display='flex', flex_flow='column', align_items='center', width='100%')\n",
    "btnContainer = ipw.HBox(children=[btnSearch], layout=box_layout)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    if len(txt_keyword.value.strip()) < 1:\n",
    "        popup(\"Enter search Key.\")\n",
    "    else:\n",
    "        main_panel.children = search(txt_keyword.value.lower())\n",
    "\n",
    "btnSearch.on_click(on_button_clicked)\n",
    "\n",
    "side_bar = ipw.VBox([txt_keyword, btnContainer]).add_class('side_bar')\n",
    "main_panel = ipw.VBox([])\n",
    "\n",
    "ipw.AppLayout(header=headerBox, left_sidebar=side_bar, center=main_panel, right_sidebar=None, footer=footerBox,\n",
    "    pane_widths=[2, 7, 0],\n",
    "    pane_heights=[1, 9, '40px'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047a929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da2d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
